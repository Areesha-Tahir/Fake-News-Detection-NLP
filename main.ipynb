{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "i181655.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Svkq9-GgqPhh",
        "outputId": "f7123200-ad0e-4cbe-b2c6-02495371da80"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "import spacy\n",
        "import math\n",
        "nlp = spacy.blank('ur')\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpqWyuEKzE3e"
      },
      "source": [
        "Using the os library path to the Train folder is being given and a list of files in the real and fake is obtained. Using that list of file names the files are being read"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ToWH2uj57my"
      },
      "source": [
        "path = \"/content/drive/My Drive/NLP/Train/Real\"\n",
        "os.chdir(path)\n",
        "real_files = os.listdir(path) \n",
        "\n",
        "real_corpus = list()\n",
        "\n",
        "for filename in real_files:\n",
        "  with open(os.path.join(path, filename), 'r') as f:\n",
        "    text = f.read()\n",
        "    real_corpus.append(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g31Lc3oFt2E"
      },
      "source": [
        "path = \"/content/drive/My Drive/NLP/Train/Fake\"\n",
        "os.chdir(path)\n",
        "fake_files = os.listdir(path)\n",
        "\n",
        "fake_corpus = list()\n",
        "\n",
        "for filename in fake_files:\n",
        "  with open(os.path.join(path, filename), 'r') as f:\n",
        "    text = f.read()\n",
        "    fake_corpus.append(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvXLOZnkywHi"
      },
      "source": [
        "In the function below the real and fake corpus is being tokenised into a list of words using spacy and vocabulary is being extracted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id0WX07RB_kE"
      },
      "source": [
        "Making Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvdWriKnB-T0"
      },
      "source": [
        "real_copy = real_corpus.copy()    #***********making copies so the actual corpus does no get modified otherwise the corpus gets modified************\n",
        "fake_copy = fake_corpus.copy()\n",
        "def extract_vocab():  #extracting vocabulary from the training corpus\n",
        "  vocab = list()  #list to contain all the words of the vocabulary \n",
        "  i = 0\n",
        "  #for real traing corpus\n",
        "  while i < len(real_corpus):  #traversing through the real news text files\n",
        "    doc = nlp(real_copy[i])   #using spacy tokenizer\n",
        "    for word in doc:\n",
        "      vocab.append(str(word))   #appending word to the vocab list\n",
        "    i += 1\n",
        "\n",
        "  #for fake training corpus\n",
        "  i = 0\n",
        "  while i < len(fake_corpus): #traversing through the fake news text files\n",
        "    doc = nlp(fake_copy[i])   #using spacy tokenizer\n",
        "    for word in doc:\n",
        "      vocab.append(str(word)) #appending word to the vocab list\n",
        "    i += 1\n",
        "  return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0UYwlKbzfyS"
      },
      "source": [
        "Making a copy of corpus just to make sure the original corpuses are not modified in any way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyG2IZwET0va"
      },
      "source": [
        "real_copy = list()  \n",
        "fake_copy = list()\n",
        "real_copy = real_corpus.copy()   \n",
        "fake_copy = fake_corpus.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCJt1Jk-zsto"
      },
      "source": [
        "To count the freq of words in both the real and fake files corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOQWKuN7Q5VK"
      },
      "source": [
        "real_dict = dict()    #dictionary to store unigram of words from the real corpus\n",
        "fake_dict = dict()    #dictionary to store unigram of words from the fake corpus\n",
        "def unigram(v): \n",
        "  i = 0\n",
        "  while i < len(real_corpus):  #traversing through the real news text files\n",
        "    doc = nlp(real_copy[i])     \n",
        "    for word in doc:\n",
        "      if str(word) in v:          #if word exists in the vocabulary\n",
        "        if str(word) not in real_dict.keys():    #if the word is not already in the dictionary \n",
        "          real_dict[str(word)] = 1      #then  initialise it by 1\n",
        "        elif str(word) in real_dict:    #if it is in the dict \n",
        "          real_dict[str(word)] += 1 #add 1 to the freq\n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  while i < len(fake_corpus): #traversing through the fake news text files\n",
        "    doc = nlp(fake_copy[i])\n",
        "    for word in doc:\n",
        "      if str(word) in v:        #if word exists in the vocabulary\n",
        "        if str(word) in fake_dict.keys():    #if the word is already in the dictionary\n",
        "          fake_dict[str(word)] += 1    #then add 1 to the count\n",
        "        else:       #if it is not in the dict \n",
        "          fake_dict[str(word)] = 1     #initialise the count by 1\n",
        "    i += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M674L2Co0cQA"
      },
      "source": [
        "Function to count all the texts in the training corpus i.e all the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0pWYP91HK0J"
      },
      "source": [
        "def count_texts(r, f):  #counting all the texts in the training corpus\n",
        "  n = len(r) + len(f) #both real and fake texts\n",
        "  return n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfvCW0se1Ehp"
      },
      "source": [
        "Function to count all the words in the class by using spacy to tokenise them and making a list of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7MGOO-V4B5R"
      },
      "source": [
        "def count_all_words(class_text):\n",
        "  words_list = list()   #to store all the word in the class\n",
        "  text = list()\n",
        "  text = class_text.copy()\n",
        "  i = 0\n",
        "  while i < len(class_text): #traversing through the class texts \n",
        "    doc = nlp(text[i])    \n",
        "    for word in doc:    \n",
        "      words_list.append(str(word))    #making a list of words by appending each word in the word list\n",
        "    i += 1\n",
        "  \n",
        "  return len(words_list) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzhDlo-I1kaX"
      },
      "source": [
        "function to count how many times a word has appeared in a particular class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWnm9VzRATZ1"
      },
      "source": [
        "def count_token_of_words(word, Doc):\n",
        "  word_count = 0    \n",
        "  word_count = Doc[word]    #Doc is a dict that contains unigrams of all the words of the class\n",
        "  \n",
        "  return word_count   #returning the total number of times has has appeared in the said class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc470RNq149M"
      },
      "source": [
        "Making dict() for conditional probability, prior and score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyLpi8Z2p3U3"
      },
      "source": [
        "cond_prob = dict()    #dictionary to store all the conditional probabilities\n",
        "cond_prob['Real'] = dict()  \n",
        "cond_prob['Fake'] = dict()\n",
        "prior = dict()  #dictionary to store prior of real and fake corpus\n",
        "score = dict()  #dictionary to store score of real and fake corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27vgDe4a2AkR"
      },
      "source": [
        "Training through the multinomial Naive Bayes theorem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr6e1s5jI_1m"
      },
      "source": [
        "def TrainMultinomialNB(v, r, f, real_cor, fake_cor, real_dic, fake_dic): \n",
        "  Vocabulary = v    #extracting vocab of the whole corpus\n",
        "  N = count_texts(r, f)   #counting all the texts in the corpus\n",
        "  \n",
        "  Nc = len(r)       #count texts in each class\n",
        "  Nw = count_all_words(real_cor)    #count words in all the texts of the said class\n",
        "  prior['Real'] = Nc/N    #calculate prior\n",
        "  Doc_c = real_dic    #dict() containing all teh counts of the words that appeared\n",
        "  for w in Vocabulary:    #if word in vocab\n",
        "    if w in Doc_c:    #and word in doc\n",
        "      Ni = count_token_of_words(w, Doc_c)   #get count of that word\n",
        "      cond_prob['Real'][w] = (Ni + 1) / (Nw + len(Vocabulary))  #calculate conditional probability\n",
        "\n",
        "  Nc = 0\n",
        "  Nw = 0\n",
        "  Doc_c = dict()\n",
        "  Nc = len(f)   #count texts in each class\n",
        "  Nw = count_all_words(fake_cor)  #count words in all the texts of the said class\n",
        "  prior['Fake'] = Nc/N    #calculate prior\n",
        "  Doc_c = fake_dic    #dict() containing all teh counts of the words that appeared\n",
        "  for w1 in Vocabulary:  #if word in vocab\n",
        "    if w1 in Doc_c:    #and word in doc\n",
        "      Ni = count_token_of_words(w1, Doc_c)     #get count of that word\n",
        "      cond_prob['Fake'][w1] = (Ni + 1) / (Nw + len(Vocabulary))    #calculate conditional probability\n",
        "  return Vocabulary, prior, cond_prob   #return vocab, prior and conditional prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXvTNQOz5foq"
      },
      "source": [
        "v = extract_vocab()   #extracting vocab\n",
        "unigram(v)    #making unigrams of both classes\n",
        "Vocab, Prior, Cond_prob = TrainMultinomialNB(v, real_files, fake_files, real_corpus, fake_corpus, real_dict, fake_dict) #training through Naive Bayes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD9mGILU3ja1"
      },
      "source": [
        "Function to extract all the words from text by using spacy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlavLKKG7w7L"
      },
      "source": [
        "def extractWordsFromText(text):\n",
        "  w_list = list()   #list to append the words in \n",
        "  doc = nlp(text)\n",
        "  for word in doc:    #for each word in the doc\n",
        "    w_list.append(str(word))    #append word into the word list\n",
        "  \n",
        "  return w_list   #return list of words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWGrsq5539UV"
      },
      "source": [
        "Applying the Multinomial Theorem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilnOi6f952F5"
      },
      "source": [
        "def ApplyMultinomialNB(Cond_prob, Prior, test):\n",
        "  Word_list = list()\n",
        "  Word_list = extractWordsFromText(test)    #getting all the words from the text\n",
        "  score['Real'] = math.log(Prior['Real'])   #get prior of the class \n",
        "  for w in Word_list:   #for each word in the list \n",
        "    if str(w) in Cond_prob['Real']:     \n",
        "      score['Real'] += math.log(Cond_prob['Real'][w])   #get its conditional prob and add it to the prior and so on \n",
        "\n",
        "  score['Fake'] = math.log(Prior['Fake'])   #get prior of the class \n",
        "  for w in Word_list:       #for each word in the list \n",
        "    if str(w) in Cond_prob['Fake']:\n",
        "      score['Fake'] += math.log(Cond_prob['Fake'][w])  #get its conditional prob and add it to the prior and so on \n",
        "\n",
        "  return score   #return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7XPdVbc_lhH"
      },
      "source": [
        "def Detection():\n",
        "  path = \"/content/drive/My Drive/NLP/Test/Real\"\n",
        "  os.chdir(path)\n",
        "  r_files = os.listdir(path)    #getting a list of all the txt file names in the test real folder\n",
        "\n",
        "\n",
        "  path2 = \"/content/drive/My Drive/NLP/Test/Fake\"\n",
        "  os.chdir(path2)\n",
        "  f_files = os.listdir(path2)       #getting a list of all the txt file names in the test fake folder\n",
        "\n",
        "  answer = list()   #list to get the actual answer after applying the algo\n",
        "  predicted = list()    #list to contain the predicted answer\n",
        "\n",
        "  for filename in r_files:      #traversing through real files\n",
        "    with open(os.path.join(path, filename), 'r') as f:    #reading all the texts from the real class\n",
        "      test = f.read()\n",
        "      final = dict()\n",
        "      final = ApplyMultinomialNB(Cond_prob, Prior, test)    #applying the multinomial algo\n",
        "      argmax = max(final, key=final.get)     #get max value of score\n",
        "      predicted.append('Real')    #predicted answers\n",
        "      answer.append(argmax)     #answer after applying algo\n",
        "\n",
        "  for filename in f_files:    #traversing through fake files\n",
        "    with open(os.path.join(path, filename), 'r') as f:    #reading all the texts from the fake class\n",
        "      test = f.read()\n",
        "      final = dict()\n",
        "      final = ApplyMultinomialNB(Cond_prob, Prior, test)    #applying the nultinomial algo\n",
        "      argmax = max(final, key=final.get)       #get max value of score\n",
        "      predicted.append('Fake')      #predicted answers\n",
        "      answer.append(argmax)       #answer after applying algo\n",
        "\n",
        "  accuracy = accuracy_score(answer, predicted)        #getting accuracy\n",
        "  precision = precision_score(answer, predicted, average = 'macro')  #getting precision\n",
        "  recall = recall_score(answer, predicted, average = 'macro')     #getting recall\n",
        "  f1 = f1_score(answer, predicted, average = 'macro')     #getting f1 mearsure\n",
        "\n",
        "  print(\"Accuracy = \", accuracy)\n",
        "  print(\"Precision = \", precision)\n",
        "  print(\"Recall = \", recall)\n",
        "  print(\"F1 = \", f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERSSzSv_2Ulu"
      },
      "source": [
        "Without Removing stopwords and duplicates from each text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvnyVJh7a60q",
        "outputId": "9cf1f173-b90e-407b-b24f-0d95d814f53d"
      },
      "source": [
        "Detection()     #Detecting real vs fake news"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy =  0.5\n",
            "Precision =  0.5056547619047619\n",
            "Recall =  0.505568255084696\n",
            "F1 =  0.4994093023934192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CMgQbf0lkTA"
      },
      "source": [
        "After removing Stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaa-ul8u50hd"
      },
      "source": [
        "Reading the stop words file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqbvsSgXX6QH"
      },
      "source": [
        "with open('/content/drive/My Drive/NLP/stopwords-ur.txt', mode = 'r') as f:\n",
        "  stopwords = f.read()\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7sqYyYmqW1D"
      },
      "source": [
        "Making a copy of corpus just to make sure the original corpuses are not modified in any way.\n",
        "\n",
        "\n",
        "Doing this again for without the stopwords and text duplicates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReuGwcV59bd_"
      },
      "source": [
        "real_copy = list()\n",
        "fake_copy = list()\n",
        "real_copy = real_corpus.copy()\n",
        "fake_copy = fake_corpus.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUCKIbZzEY_S"
      },
      "source": [
        "Extracting vocabulary and removing stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArvAHqPYmlJI"
      },
      "source": [
        "def extract_vocab_no_stopwords():  #extracting vocabulary from the training corpus\n",
        "  vocab = list()  #list to contain all the words of the vocabulary \n",
        "  i = 0\n",
        "  while i < len(real_corpus):  #traversing through the real news text files\n",
        "    doc = nlp(real_copy[i])\n",
        "    for word in doc:\n",
        "      if str(word) not in stopwords:    #if word is not in the stopwords list only then add to the vocab\n",
        "        vocab.append(str(word))\n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  while i < len(fake_corpus): #traversing through the fake news text files\n",
        "    doc = nlp(fake_copy[i])\n",
        "    for word in doc:\n",
        "      if str(word) not in stopwords:    #if word is not in the stopwords list only then add to the vocab\n",
        "        vocab.append(str(word))\n",
        "    i += 1\n",
        "  return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtSJIbxmGVTl"
      },
      "source": [
        "Making unigram of the corpuses without stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-_iP_0rF8Zv"
      },
      "source": [
        "real_dict = dict()    #dictionary to store unigram of words from the real corpus\n",
        "fake_dict = dict()    #dictionary to store unigram of words from the fake corpus\n",
        "real_copy = list()\n",
        "fake_copy = list()\n",
        "real_copy = real_corpus.copy()\n",
        "fake_copy = fake_corpus.copy()\n",
        "def unigram_without_sw(v):  \n",
        "  i = 0\n",
        "  while i < len(real_corpus):  #traversing through the real news text files\n",
        "    doc = nlp(real_copy[i])     \n",
        "    for word in doc:\n",
        "      if str(word) not in stopwords:\n",
        "        if str(word) in v:          #if word exists in the vocabulary\n",
        "          if str(word) not in real_dict.keys():    #if the word is not already in the dictionary \n",
        "            real_dict[str(word)] = 1      #then  initialise it by 1\n",
        "          elif str(word) in real_dict:    #if it is in the dict \n",
        "            real_dict[str(word)] += 1 #add 1 to the freq\n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  while i < len(fake_corpus): #traversing through the fake news text files\n",
        "    doc = nlp(fake_copy[i])\n",
        "    for word in doc:\n",
        "      if str(word) not in stopwords:\n",
        "        if str(word) in v:        #if word exists in the vocabulary\n",
        "          if str(word) in fake_dict.keys():    #if the word is already in the dictionary\n",
        "            fake_dict[str(word)] += 1    #then add 1 to the count\n",
        "          else:       #if it is not in the dict \n",
        "            fake_dict[str(word)] = 1     #initialise the count by 1\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H08fA3ujFqOe"
      },
      "source": [
        "With only stopwords words removed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_jK8pGNCmFX",
        "outputId": "081d5de4-b730-4b59-daa9-cb3f35c7ae80"
      },
      "source": [
        "cond_prob = dict()    #dictionary to store all the conditional probabilities\n",
        "cond_prob['Real'] = dict()  \n",
        "cond_prob['Fake'] = dict()\n",
        "prior = dict()  #dictionary to store prior of real and fake corpus\n",
        "score = dict()  #dictionary to store score of real and fake corpus\n",
        "v = extract_vocab_no_stopwords()   #extracting vocab\n",
        "unigram_without_sw(v)    #making unigrams of both classes\n",
        "Vocab, Prior, Cond_prob = TrainMultinomialNB(v, real_files, fake_files, real_corpus, fake_corpus, real_dict, fake_dict) #training through Naive Bayes\n",
        "Detection()     #Detection of real vs fake news using Boolean Naive Bayes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy =  0.46946564885496184\n",
            "Precision =  0.5038690476190476\n",
            "Recall =  0.5048809791995195\n",
            "F1 =  0.45477954936746756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2ExiYAREhrf"
      },
      "source": [
        "Removing duplicates from each text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzdfdtQ2DNQc"
      },
      "source": [
        "real_copy = list()\n",
        "fake_copy = list()\n",
        "real_copy = real_corpus.copy()\n",
        "fake_copy = fake_corpus.copy()\n",
        "noDup_real = dict()   #to store count of each word appearing in a real texts\n",
        "noDup_fake = dict()   #to store count of each word appearing in a fake texts\n",
        "real_corpus_noDup = list()    #list to store real text after removal of duplicates\n",
        "fake_corpus_noDup = list()    #list to store fake text after removal of duplicates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKdmTqUrudHX"
      },
      "source": [
        "def remove_duplicates():\n",
        "  i = 0\n",
        "  while i < len(real_corpus):     #traversing through real corpus\n",
        "    w_list = real_copy[i].split()   #splitting the text into words\n",
        "    done = ''\n",
        "    for word in w_list:   \n",
        "      if word not in done:    #if word not already visited than add to list\n",
        "        if word in noDup_real.keys():   #if word already exists in the dict because it might have been present in the last text than add 1 to count\n",
        "          noDup_real[word] += 1\n",
        "        else:\n",
        "          noDup_real[word] = 1          #else just initialise word count by 1    \n",
        "        done = done + word + ' '        #concatenate words together for form the text without any duplicates\n",
        "    real_corpus_noDup.append(done)      #append to real corpus texts list\n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  while i < len(fake_corpus):   #traversing through fake corpus\n",
        "    w_list1 = fake_copy[i].split()      #splitting the text into words\n",
        "    done = ''\n",
        "    for word in w_list1:\n",
        "      if word not in done:   #if word not already visited than add to list\n",
        "        if word in noDup_fake.keys():   #if word already exists in the dict because it might have been present in the last text than add 1 to count\n",
        "          noDup_fake[word] += 1\n",
        "        else:\n",
        "          noDup_fake[word] = 1          #else just initialise word count by 1\n",
        "        done = done + word + ' '        #concatenate words together for form the text without any duplicates\n",
        "    fake_corpus_noDup.append(done)      #append to fake corpus texts list\n",
        "    i += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnmE2Smo1Ycj"
      },
      "source": [
        "BOOLEAN NAIVE BAYES\n",
        "\n",
        "(With stopwords but duplicates removed from the coupus)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX09oIIezPVb",
        "outputId": "2765561c-4101-4418-96a2-1dcfd656101a"
      },
      "source": [
        "cond_prob = dict()    #dictionary to store all the conditional probabilities\n",
        "cond_prob['Real'] = dict()  \n",
        "cond_prob['Fake'] = dict()\n",
        "real_copy = real_corpus.copy()\n",
        "fake_copy = fake_corpus.copy()\n",
        "prior = dict()  #dictionary to store prior of real and fake corpus\n",
        "score = dict()  #dictionary to store score of real and fake corpus\n",
        "v = extract_vocab()\n",
        "remove_duplicates()     #remove duplicates from each text\n",
        "Vocab, Prior, Cond_prob = TrainMultinomialNB(v, real_files, fake_files, real_corpus_noDup, fake_corpus_noDup, noDup_real, noDup_fake)  #Training through multinomial Naive Bayes\n",
        "Detection()     #Detection of real vs fake news using Boolean Naive Bayes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy =  0.4580152671755725\n",
            "Precision =  0.49386904761904765\n",
            "Recall =  0.49203772418058134\n",
            "F1 =  0.4407768157768158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpIqrrXl2A8L"
      },
      "source": [
        "Removing Stopwords and duplicates from each text (Basically removing tsopwords from Boolean Naive Bayes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9zTczoC1qQR"
      },
      "source": [
        "def remove_duplicates_and_stopwords():\n",
        "  i = 0\n",
        "  while i < len(real_corpus):     #traversing through real corpus\n",
        "    w_list = real_copy[i].split()   #splitting the text into words\n",
        "    done = ''\n",
        "    for word in w_list: \n",
        "      if word not in stopwords:  #if word in not in the stopwords list\n",
        "        if word not in done:    #if word not already visited than add to list\n",
        "          if word in noDup_real.keys():   #if word already exists in the dict because it might have been present in the last text than add 1 to count\n",
        "            noDup_real[word] += 1\n",
        "          else:\n",
        "            noDup_real[word] = 1          #else just initialise word count by 1    \n",
        "          done = done + word + ' '        #concatenate words together for form the text without any duplicates\n",
        "    real_corpus_noDup.append(done)      #append to real corpus texts list\n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  while i < len(fake_corpus):   #traversing through fake corpus\n",
        "    w_list1 = fake_copy[i].split()      #splitting the text into words\n",
        "    done = ''\n",
        "    for word in w_list1:\n",
        "      if word not in stopwords:  #if word in not in the stopwords list\n",
        "        if word not in done:   #if word not already visited than add to list\n",
        "          if word in noDup_fake.keys():   #if word already exists in the dict because it might have been present in the last text than add 1 to count\n",
        "            noDup_fake[word] += 1\n",
        "          else:\n",
        "            noDup_fake[word] = 1          #else just initialise word count by 1\n",
        "          done = done + word + ' '        #concatenate words together for form the text without any duplicates\n",
        "    fake_corpus_noDup.append(done)      #append to fake corpus texts list\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbJ5zzH5vN8x"
      },
      "source": [
        "cond_prob = dict()    #dictionary to store all the conditional probabilities\n",
        "cond_prob['Real'] = dict()  \n",
        "cond_prob['Fake'] = dict()\n",
        "prior = dict()  #dictionary to store prior of real and fake corpus\n",
        "score = dict()  #dictionary to store score of real and fake corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4Yq3KjW7kWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d7b8420-6de9-45c3-b8ae-e4f3ff327d86"
      },
      "source": [
        "v = extract_vocab_no_stopwords()    #extracting vocab without any stopwords in it\n",
        "remove_duplicates_and_stopwords()     #remove duplicates from each text\n",
        "Vocab, Prior, Cond_prob = TrainMultinomialNB(v, real_files, fake_files, real_corpus_noDup, fake_corpus_noDup, noDup_real, noDup_fake)  #Training through multinomial Naive Bayes\n",
        "Detection()  #Detection of real vs fake news after stopwords and duplicates"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy =  0.4580152671755725\n",
            "Precision =  0.49839285714285714\n",
            "Recall =  0.4977180527383367\n",
            "F1 =  0.4339703018500487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_C-sbO-vZu2"
      },
      "source": [
        "\n",
        "**CASES:**\n",
        "\n",
        "\n",
        "1.   Without Removing stopwords and duplicates\n",
        "2.   Removing stopwords only\n",
        "\n",
        "1.   Removing Duplicates only\n",
        "2.   Removing Both Stopwords and Duplicates\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}